{
  "created_at": "2025-10-13 18:17:15",
  "model_name": "masked_gnn_attention_model",
  "custom_model_config": {
    "lane_mlp_hiddens": [
      32
    ],
    "lane_activation": "ReLU",
    "lane_attn_neurons": 64,
    "node_dim": 128,
    "gnn_layers": 2,
    "gnn_activation": "ReLU",
    "use_self_loop": true,
    "actor_hiddens": [
      128,
      128
    ],
    "critic_hiddens": [
      128,
      128
    ]
  },
  "checkpoint_path": "checkpoint",
  "env_kwargs_file": "env_kwargs.pkl",
  "notes": ""
}